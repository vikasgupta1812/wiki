{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: gbm\n",
      "Loading required package: survival\n",
      "Loading required package: lattice\n",
      "Loading required package: splines\n",
      "Loading required package: parallel\n",
      "Loaded gbm 2.1.1\n"
     ]
    }
   ],
   "source": [
    "require(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for gbm {gbm}\"><tr><td>gbm {gbm}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Generalized Boosted Regression Modeling</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Fits generalized boosted regression models.</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "gbm(formula = formula(data),\n",
       "    distribution = \"bernoulli\",\n",
       "    data = list(),\n",
       "    weights,\n",
       "    var.monotone = NULL,\n",
       "    n.trees = 100,\n",
       "    interaction.depth = 1,\n",
       "    n.minobsinnode = 10,\n",
       "    shrinkage = 0.001,\n",
       "    bag.fraction = 0.5,\n",
       "    train.fraction = 1.0,\n",
       "    cv.folds=0,\n",
       "    keep.data = TRUE,\n",
       "    verbose = \"CV\",\n",
       "    class.stratify.cv=NULL,\n",
       "    n.cores = NULL)\n",
       "\n",
       "gbm.fit(x, y,\n",
       "        offset = NULL,\n",
       "        misc = NULL,\n",
       "        distribution = \"bernoulli\",\n",
       "        w = NULL,\n",
       "        var.monotone = NULL,\n",
       "        n.trees = 100,\n",
       "        interaction.depth = 1,\n",
       "        n.minobsinnode = 10,\n",
       "        shrinkage = 0.001,\n",
       "        bag.fraction = 0.5,\n",
       "        nTrain = NULL,\n",
       "        train.fraction = NULL,\n",
       "        keep.data = TRUE,\n",
       "        verbose = TRUE,\n",
       "        var.names = NULL,\n",
       "        response.name = \"y\",\n",
       "        group = NULL)\n",
       "\n",
       "gbm.more(object,\n",
       "         n.new.trees = 100,\n",
       "         data = NULL,\n",
       "         weights = NULL,\n",
       "         offset = NULL,\n",
       "         verbose = NULL)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>formula</code></td>\n",
       "<td>\n",
       "<p>a symbolic description of the model to be fit. The formula may include an offset term (e.g. y~offset(n)+x). If <code>keep.data=FALSE</code> in the initial call to <code>gbm</code> then it is the user's responsibility to resupply the offset to <code>gbm.more</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>distribution</code></td>\n",
       "<td>\n",
       "<p>either a character string specifying the name of the distribution to use or a list with a component <code>name</code> specifying the distribution and any additional parameters needed. If not specified, <code>gbm</code> will try to guess: if the response has only 2 unique values, bernoulli is assumed; otherwise, if the response is a factor, multinomial is assumed; otherwise, if the response has class &quot;Surv&quot;, coxph is assumed; otherwise, gaussian is assumed.\n",
       "</p>\n",
       "<p>Currently available options are &quot;gaussian&quot; (squared error), &quot;laplace&quot; (absolute loss), &quot;tdist&quot; (t-distribution loss), &quot;bernoulli&quot; (logistic regression for 0-1 outcomes),\n",
       "&quot;huberized&quot; (huberized hinge loss for 0-1 outcomes),\n",
       "&quot;multinomial&quot; (classification when there are more than 2 classes), &quot;adaboost&quot; (the AdaBoost exponential loss for 0-1 outcomes), &quot;poisson&quot; (count outcomes), &quot;coxph&quot; (right censored observations), &quot;quantile&quot;, or &quot;pairwise&quot; (ranking measure using the LambdaMart algorithm).\n",
       "</p>\n",
       "<p>If quantile regression is specified, <code>distribution</code> must be a list of the form <code>list(name=\"quantile\",alpha=0.25)</code> where <code>alpha</code> is the quantile to estimate. The current version's quantile regression method does not handle non-constant weights and will stop.\n",
       "</p>\n",
       "<p>If &quot;tdist&quot; is specified, the default degrees of freedom is 4 and this can be controlled by specifying <code>distribution=list(name=\"tdist\", df=DF)</code> where <code>DF</code> is your chosen degrees of freedom.\n",
       "</p>\n",
       "<p>If &quot;pairwise&quot; regression is specified, <code>distribution</code> must be a list\n",
       "of the form \n",
       "<code>list(name=\"pairwise\",group=...,metric=...,max.rank=...)</code>\n",
       "(<code>metric</code> and <code>max.rank</code> are optional, see\n",
       "below). <code>group</code> is a character vector with the column names of\n",
       "<code>data</code> that jointly indicate the group an instance belongs to\n",
       "(typically a query in Information Retrieval  applications).\n",
       "For training, only pairs of\n",
       "instances from the same group and with different target labels can be\n",
       "considered. <code>metric</code> is the IR measure to use, one of \n",
       "</p>\n",
       "\n",
       "<dl>\n",
       "<dt><code>conc</code>:</dt><dd><p>Fraction of concordant pairs; for binary labels,\n",
       "this is equivalent to the Area under the ROC Curve</p>\n",
       "</dd>\n",
       "<dt><code>mrr</code>:</dt><dd><p>Mean reciprocal rank of the highest-ranked positive instance</p>\n",
       "</dd>\n",
       "<dt><code>map</code>:</dt><dd><p>Mean average precision, a generalization of\n",
       "<code>mrr</code> to multiple positive instances</p>\n",
       "</dd>\n",
       "<dt><code>ndcg:</code></dt><dd><p>Normalized discounted cumulative gain. The score is\n",
       "the weighted sum (DCG) of the user-supplied target values, weighted by\n",
       "log(rank+1), and normalized to the maximum achievable value. This \n",
       "is the default if the user did not specify a metric.</p>\n",
       "</dd> \n",
       "</dl>\n",
       "\n",
       "<p><code>ndcg</code> and <code>conc</code> allow arbitrary target values, while binary\n",
       "targets {0,1} are expected for <code>map</code> and <code>mrr</code>. For\n",
       "<code>ndcg</code> and <code>mrr</code>, a cut-off can be chosen using a positive\n",
       "integer parameter <code>max.rank</code>. If left unspecified, all ranks are\n",
       "taken into account.\n",
       "</p>\n",
       "<p>Note that splitting of instances into training and validation sets\n",
       "follows group boundaries and therefore only approximates the specified\n",
       "<code>train.fraction</code> ratio (the same applies to cross-validation\n",
       "folds). Internally, queries are randomly shuffled before training, to\n",
       "avoid bias.\n",
       "</p>\n",
       "<p>Weights can be used in conjunction with pairwise metrics, however it is\n",
       "assumed that they are constant for instances from the same group.\n",
       "</p>\n",
       "<p>For details and background on the algorithm, see e.g. Burges (2010).\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>data</code></td>\n",
       "<td>\n",
       "<p>an optional data frame containing the variables in the model. By default the variables are taken from <code>environment(formula)</code>, typically the environment from which <code>gbm</code> is called. If <code>keep.data=TRUE</code> in the initial call to <code>gbm</code> then <code>gbm</code> stores a copy with the object. If <code>keep.data=FALSE</code> then subsequent calls to <code>gbm.more</code> must resupply the same dataset. It becomes the user's responsibility to resupply the same data at this point.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>weights</code></td>\n",
       "<td>\n",
       "<p>an optional vector of weights to be used in the fitting process. Must be positive but do not need to be normalized. If <code>keep.data=FALSE</code> in the initial call to <code>gbm</code> then it is the user's responsibility to resupply the weights to <code>gbm.more</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>var.monotone</code></td>\n",
       "<td>\n",
       "<p>an optional vector, the same length as the number of predictors, indicating which variables have a monotone increasing (+1), decreasing (-1), or arbitrary (0) relationship with the outcome.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.trees</code></td>\n",
       "<td>\n",
       "<p>the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cv.folds</code></td>\n",
       "<td>\n",
       "<p>Number of cross-validation folds to perform. If <code>cv.folds</code>&gt;1 then <code>gbm</code>, in addition to the usual fit, will perform a cross-validation, calculate an estimate of generalization error returned in <code>cv.error</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>interaction.depth</code></td>\n",
       "<td>\n",
       "<p>The maximum depth of variable interactions. 1 implies an additive model, 2 implies a model with up to 2-way interactions, etc.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.minobsinnode</code></td>\n",
       "<td>\n",
       "<p>minimum number of observations in the trees terminal nodes. Note that this is the actual number of observations not the total weight.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>shrinkage</code></td>\n",
       "<td>\n",
       "<p>a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>bag.fraction</code></td>\n",
       "<td>\n",
       "<p>the fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice will result in similar but different fits. <code>gbm</code> uses the R random number generator so <code>set.seed</code> can ensure that the model can be reconstructed. Preferably, the user can save the returned <code>gbm.object</code> using <code>save</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>train.fraction</code></td>\n",
       "<td>\n",
       "<p>The first <code>train.fraction * nrows(data)</code>\n",
       "observations are used to fit the <code>gbm</code> and the remainder are used\n",
       "for computing out-of-sample estimates of the loss function.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nTrain</code></td>\n",
       "<td>\n",
       "<p>An integer representing the number of cases on which to\n",
       "train. This is the preferred way of specification for <code>gbm.fit</code>;\n",
       "The option <code>train.fraction</code> in <code>gbm.fit</code> is deprecated and\n",
       "only maintained for backward compatibility. These two parameters are\n",
       "mutually exclusive. If both are unspecified, all data is used for training.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>keep.data</code></td>\n",
       "<td>\n",
       "<p>a logical variable indicating whether to keep the data and an index of the data stored with the object. Keeping the data and index makes subsequent calls to <code>gbm.more</code> faster at the cost of storing an extra copy of the dataset.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>object</code></td>\n",
       "<td>\n",
       "<p>a <code>gbm</code> object created from an initial call to <code>gbm</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.new.trees</code></td>\n",
       "<td>\n",
       "<p>the number of additional trees to add to <code>object</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>verbose</code></td>\n",
       "<td>\n",
       "<p>If TRUE, gbm will print out progress and performance indicators. If this option is left unspecified for gbm.more then it uses <code>verbose</code> from <code>object</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>class.stratify.cv</code></td>\n",
       "<td>\n",
       "<p>whether or not the cross-validation should be stratified by class. Defaults to <code>TRUE</code> for <code>distribution=\"multinomial\"</code> and is only implementated for <code>multinomial</code> and <code>bernoulli</code>. The purpose of stratifying the cross-validation is to help avoiding situations in which training sets do not contain all classes.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>x, y</code></td>\n",
       "<td>\n",
       "<p>For <code>gbm.fit</code>: <code>x</code> is a data frame or data matrix containing the predictor variables and <code>y</code> is the vector of outcomes. The number of rows in <code>x</code> must be the same as the length of <code>y</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>a vector of values for the offset</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>misc</code></td>\n",
       "<td>\n",
       "<p>For <code>gbm.fit</code>: <code>misc</code> is an R object that is simply passed on to the gbm engine. It can be used for additional data for the specific distribution. Currently it is only used for passing the censoring indicator for the Cox proportional hazards model.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>w</code></td>\n",
       "<td>\n",
       "<p>For <code>gbm.fit</code>: <code>w</code> is a vector of weights of the same length as the <code>y</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>var.names</code></td>\n",
       "<td>\n",
       "<p>For <code>gbm.fit</code>: A vector of strings of length equal to the number of columns of <code>x</code> containing the names of the predictor variables.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>response.name</code></td>\n",
       "<td>\n",
       "<p>For <code>gbm.fit</code>: A character string label for the response variable.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>group</code></td>\n",
       "<td>\n",
       "<p><code>group</code> used when <code>distribution = 'pairwise'.</code></p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n.cores</code></td>\n",
       "<td>\n",
       "<p>The number of CPU cores to use. The cross-validation loop\n",
       "will attempt to send different CV folds off to different cores. If\n",
       "<code>n.cores</code> is not specified by the user, it is guessed using the\n",
       "<code>detectCores</code> function in the <code>parallel</code> package. Note that\n",
       "the documentation for <code>detectCores</code> makes clear that it is not\n",
       "failsave and could return a spurious number of available cores.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>See the <a href=\"../doc/gbm.pdf\">gbm vignette</a> for technical details.\n",
       "</p>\n",
       "<p>This package implements the generalized boosted modeling framework. Boosting is the process of iteratively adding basis functions in a greedy fashion so that each additional basis function further reduces the selected loss function. This implementation closely follows Friedman's Gradient Boosting Machine (Friedman, 2001).\n",
       "</p>\n",
       "<p>In addition to many of the features documented in the Gradient Boosting Machine, <code>gbm</code> offers additional features including the out-of-bag estimator for the optimal number of iterations, the ability to store and manipulate the resulting <code>gbm</code> object, and a variety of other loss functions that had not previously had associated boosting algorithms, including the Cox partial likelihood for censored data, the poisson likelihood for count outcomes, and a gradient boosting implementation to minimize the AdaBoost exponential loss function.\n",
       "</p>\n",
       "<p><code>gbm.fit</code> provides the link between R and the C++ gbm engine. <code>gbm</code> is a front-end to <code>gbm.fit</code> that uses the familiar R modeling formulas. However, <code>model.frame</code> is very slow if there are many predictor variables. For power-users with many variables use <code>gbm.fit</code>. For general practice <code>gbm</code> is preferable.</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       " <p><code>gbm</code>, <code>gbm.fit</code>, and <code>gbm.more</code> return a <code>gbm.object</code>. </p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Greg Ridgeway <a href=\"mailto:gregridgeway@gmail.com\">gregridgeway@gmail.com</a>\n",
       "</p>\n",
       "<p>Quantile regression code developed by Brian Kriegler <a href=\"mailto:bk@stat.ucla.edu\">bk@stat.ucla.edu</a>\n",
       "</p>\n",
       "<p>t-distribution, and multinomial code developed by Harry Southworth and Daniel Edwards\n",
       "</p>\n",
       "<p>Pairwise code developed by Stefan Schroedl <a href=\"mailto:schroedl@a9.com\">schroedl@a9.com</a></p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Y. Freund and R.E. Schapire (1997) &ldquo;A decision-theoretic generalization of on-line learning and an application to boosting,&rdquo; <em>Journal of Computer and System Sciences,</em> 55(1):119-139.\n",
       "</p>\n",
       "<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science and Statistics</em> 31:172-181.\n",
       "</p>\n",
       "<p>J.H. Friedman, T. Hastie, R. Tibshirani (2000). &ldquo;Additive Logistic Regression: a Statistical View of Boosting,&rdquo; <em>Annals of Statistics</em> 28(2):337-374.\n",
       "</p>\n",
       "<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient Boosting Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.\n",
       "</p>\n",
       "<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo; <em>Computational Statistics and Data Analysis</em> 38(4):367-378.\n",
       "</p>\n",
       "<p>B. Kriegler (2007). <a href=\"http://statistics.ucla.edu/theses/uclastat-dissertation-2007:2\">Cost-Sensitive Stochastic Gradient Boosting Within a Quantitative Regression Framework</a>. PhD dissertation, UCLA Statistics.\n",
       "</p>\n",
       "<p>C. Burges (2010). &ldquo;From RankNet to LambdaRank to LambdaMART: An Overview,&rdquo; Microsoft Research Technical Report MSR-TR-2010-82.\n",
       "</p>\n",
       "<p><a href=\"http://sites.google.com/site/gregridgeway\">Greg Ridgeway's site</a>.\n",
       "</p>\n",
       "<p>The <a href=\"http://www-stat.stanford.edu/~jhf/R-MART.html\">MART</a> website. </p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       " <p><code>gbm.object</code>, <code>gbm.perf</code>, <code>plot.gbm</code>,\n",
       "<code>predict.gbm</code>, <code>summary.gbm</code>, <code>pretty.gbm.tree</code>. </p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre> # A least squares regression example # create some data\n",
       "\n",
       "N &lt;- 1000\n",
       "X1 &lt;- runif(N)\n",
       "X2 &lt;- 2*runif(N)\n",
       "X3 &lt;- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])\n",
       "X4 &lt;- factor(sample(letters[1:6],N,replace=TRUE))\n",
       "X5 &lt;- factor(sample(letters[1:3],N,replace=TRUE))\n",
       "X6 &lt;- 3*runif(N) \n",
       "mu &lt;- c(-1,0,1,2)[as.numeric(X3)]\n",
       "\n",
       "SNR &lt;- 10 # signal-to-noise ratio\n",
       "Y &lt;- X1**1.5 + 2 * (X2**.5) + mu\n",
       "sigma &lt;- sqrt(var(Y)/SNR)\n",
       "Y &lt;- Y + rnorm(N,0,sigma)\n",
       "\n",
       "# introduce some missing values\n",
       "X1[sample(1:N,size=500)] &lt;- NA\n",
       "X4[sample(1:N,size=300)] &lt;- NA\n",
       "\n",
       "data &lt;- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
       "\n",
       "# fit initial model\n",
       "gbm1 &lt;-\n",
       "gbm(Y~X1+X2+X3+X4+X5+X6,         # formula\n",
       "    data=data,                   # dataset\n",
       "    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,\n",
       "                                 # +1: monotone increase,\n",
       "                                 #  0: no monotone restrictions\n",
       "    distribution=\"gaussian\",     # see the help for other choices\n",
       "    n.trees=1000,                # number of trees\n",
       "    shrinkage=0.05,              # shrinkage or learning rate,\n",
       "                                 # 0.001 to 0.1 usually work\n",
       "    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.\n",
       "    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best\n",
       "    train.fraction = 0.5,        # fraction of data for training,\n",
       "                                 # first train.fraction*N used for training\n",
       "    n.minobsinnode = 10,         # minimum total weight needed in each node\n",
       "    cv.folds = 3,                # do 3-fold cross-validation\n",
       "    keep.data=TRUE,              # keep a copy of the dataset with the object\n",
       "    verbose=FALSE,               # don't print out progress\n",
       "    n.cores=1)                   # use only a single core (detecting #cores is\n",
       "                                 # error-prone, so avoided here)\n",
       "\n",
       "# check performance using an out-of-bag estimator\n",
       "# OOB underestimates the optimal number of iterations\n",
       "best.iter &lt;- gbm.perf(gbm1,method=\"OOB\")\n",
       "print(best.iter)\n",
       "\n",
       "# check performance using a 50% heldout test set\n",
       "best.iter &lt;- gbm.perf(gbm1,method=\"test\")\n",
       "print(best.iter)\n",
       "\n",
       "# check performance using 5-fold cross-validation\n",
       "best.iter &lt;- gbm.perf(gbm1,method=\"cv\")\n",
       "print(best.iter)\n",
       "\n",
       "# plot the performance # plot variable influence\n",
       "summary(gbm1,n.trees=1)         # based on the first tree\n",
       "summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees\n",
       "\n",
       "# compactly print the first and last trees for curiosity\n",
       "print(pretty.gbm.tree(gbm1,1))\n",
       "print(pretty.gbm.tree(gbm1,gbm1$n.trees))\n",
       "\n",
       "# make some new data\n",
       "N &lt;- 1000\n",
       "X1 &lt;- runif(N)\n",
       "X2 &lt;- 2*runif(N)\n",
       "X3 &lt;- ordered(sample(letters[1:4],N,replace=TRUE))\n",
       "X4 &lt;- factor(sample(letters[1:6],N,replace=TRUE))\n",
       "X5 &lt;- factor(sample(letters[1:3],N,replace=TRUE))\n",
       "X6 &lt;- 3*runif(N) \n",
       "mu &lt;- c(-1,0,1,2)[as.numeric(X3)]\n",
       "\n",
       "Y &lt;- X1**1.5 + 2 * (X2**.5) + mu + rnorm(N,0,sigma)\n",
       "\n",
       "data2 &lt;- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
       "\n",
       "# predict on the new data using \"best\" number of trees\n",
       "# f.predict generally will be on the canonical scale (logit,log,etc.)\n",
       "f.predict &lt;- predict(gbm1,data2,best.iter)\n",
       "\n",
       "# least squares error\n",
       "print(sum((data2$Y-f.predict)^2))\n",
       "\n",
       "# create marginal plots\n",
       "# plot variable X1,X2,X3 after \"best\" iterations\n",
       "par(mfrow=c(1,3))\n",
       "plot(gbm1,1,best.iter)\n",
       "plot(gbm1,2,best.iter)\n",
       "plot(gbm1,3,best.iter)\n",
       "par(mfrow=c(1,1))\n",
       "# contour plot of variables 1 and 2 after \"best\" iterations\n",
       "plot(gbm1,1:2,best.iter)\n",
       "# lattice plot of variables 2 and 3\n",
       "plot(gbm1,2:3,best.iter)\n",
       "# lattice plot of variables 3 and 4\n",
       "plot(gbm1,3:4,best.iter)\n",
       "\n",
       "# 3-way plots\n",
       "plot(gbm1,c(1,2,6),best.iter,cont=20)\n",
       "plot(gbm1,1:3,best.iter)\n",
       "plot(gbm1,2:4,best.iter)\n",
       "plot(gbm1,3:5,best.iter)\n",
       "\n",
       "# do another 100 iterations\n",
       "gbm2 &lt;- gbm.more(gbm1,100,\n",
       "                 verbose=FALSE) # stop printing detailed progress\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>gbm</em> version 2.1.1 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{gbm}{Generalized Boosted Regression Modeling}{gbm}\n",
       "\\methaliasA{gbm.fit}{gbm}{gbm.fit}\n",
       "\\methaliasA{gbm.more}{gbm}{gbm.more}\n",
       "\\keyword{models}{gbm}\n",
       "\\keyword{nonlinear}{gbm}\n",
       "\\keyword{survival}{gbm}\n",
       "\\keyword{nonparametric}{gbm}\n",
       "\\keyword{tree}{gbm}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Fits generalized boosted regression models.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "gbm(formula = formula(data),\n",
       "    distribution = \"bernoulli\",\n",
       "    data = list(),\n",
       "    weights,\n",
       "    var.monotone = NULL,\n",
       "    n.trees = 100,\n",
       "    interaction.depth = 1,\n",
       "    n.minobsinnode = 10,\n",
       "    shrinkage = 0.001,\n",
       "    bag.fraction = 0.5,\n",
       "    train.fraction = 1.0,\n",
       "    cv.folds=0,\n",
       "    keep.data = TRUE,\n",
       "    verbose = \"CV\",\n",
       "    class.stratify.cv=NULL,\n",
       "    n.cores = NULL)\n",
       "\n",
       "gbm.fit(x, y,\n",
       "        offset = NULL,\n",
       "        misc = NULL,\n",
       "        distribution = \"bernoulli\",\n",
       "        w = NULL,\n",
       "        var.monotone = NULL,\n",
       "        n.trees = 100,\n",
       "        interaction.depth = 1,\n",
       "        n.minobsinnode = 10,\n",
       "        shrinkage = 0.001,\n",
       "        bag.fraction = 0.5,\n",
       "        nTrain = NULL,\n",
       "        train.fraction = NULL,\n",
       "        keep.data = TRUE,\n",
       "        verbose = TRUE,\n",
       "        var.names = NULL,\n",
       "        response.name = \"y\",\n",
       "        group = NULL)\n",
       "\n",
       "gbm.more(object,\n",
       "         n.new.trees = 100,\n",
       "         data = NULL,\n",
       "         weights = NULL,\n",
       "         offset = NULL,\n",
       "         verbose = NULL)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{formula}] a symbolic description of the model to be fit. The formula may include an offset term (e.g. y\\textasciitilde{}offset(n)+x). If \\code{keep.data=FALSE} in the initial call to \\code{gbm} then it is the user's responsibility to resupply the offset to \\code{\\LinkA{gbm.more}{gbm.more}}.\n",
       "\\item[\\code{distribution}] either a character string specifying the name of the distribution to use or a list with a component \\code{name} specifying the distribution and any additional parameters needed. If not specified, \\code{gbm} will try to guess: if the response has only 2 unique values, bernoulli is assumed; otherwise, if the response is a factor, multinomial is assumed; otherwise, if the response has class \"Surv\", coxph is assumed; otherwise, gaussian is assumed.\n",
       "\n",
       "Currently available options are \"gaussian\" (squared error), \"laplace\" (absolute loss), \"tdist\" (t-distribution loss), \"bernoulli\" (logistic regression for 0-1 outcomes),\n",
       "\"huberized\" (huberized hinge loss for 0-1 outcomes),\n",
       "\"multinomial\" (classification when there are more than 2 classes), \"adaboost\" (the AdaBoost exponential loss for 0-1 outcomes), \"poisson\" (count outcomes), \"coxph\" (right censored observations), \"quantile\", or \"pairwise\" (ranking measure using the LambdaMart algorithm).\n",
       "\n",
       "If quantile regression is specified, \\code{distribution} must be a list of the form \\code{list(name=\"quantile\",alpha=0.25)} where \\code{alpha} is the quantile to estimate. The current version's quantile regression method does not handle non-constant weights and will stop.\n",
       "\n",
       "If \"tdist\" is specified, the default degrees of freedom is 4 and this can be controlled by specifying \\code{distribution=list(name=\"tdist\", df=DF)} where \\code{DF} is your chosen degrees of freedom.\n",
       "\n",
       "If \"pairwise\" regression is specified, \\code{distribution} must be a list\n",
       "of the form \n",
       "\\code{list(name=\"pairwise\",group=...,metric=...,max.rank=...)}\n",
       "(\\code{metric} and \\code{max.rank} are optional, see\n",
       "below). \\code{group} is a character vector with the column names of\n",
       "\\code{data} that jointly indicate the group an instance belongs to\n",
       "(typically a query in Information Retrieval  applications).\n",
       "For training, only pairs of\n",
       "instances from the same group and with different target labels can be\n",
       "considered. \\code{metric} is the IR measure to use, one of \n",
       "\\begin{description}\n",
       "\n",
       "\\item[\\code{conc}:] Fraction of concordant pairs; for binary labels,\n",
       "this is equivalent to the Area under the ROC Curve\n",
       "\\item[\\code{mrr}:] Mean reciprocal rank of the highest-ranked positive instance\n",
       "\\item[\\code{map}:] Mean average precision, a generalization of\n",
       "\\code{mrr} to multiple positive instances\n",
       "\\item[\\code{ndcg:}] Normalized discounted cumulative gain. The score is\n",
       "the weighted sum (DCG) of the user-supplied target values, weighted by\n",
       "log(rank+1), and normalized to the maximum achievable value. This \n",
       "is the default if the user did not specify a metric.\n",
       "\n",
       "\\end{description}\n",
       "\n",
       "\n",
       "\\code{ndcg} and \\code{conc} allow arbitrary target values, while binary\n",
       "targets \\{0,1\\} are expected for \\code{map} and \\code{mrr}. For\n",
       "\\code{ndcg} and \\code{mrr}, a cut-off can be chosen using a positive\n",
       "integer parameter \\code{max.rank}. If left unspecified, all ranks are\n",
       "taken into account.\n",
       "\n",
       "Note that splitting of instances into training and validation sets\n",
       "follows group boundaries and therefore only approximates the specified\n",
       "\\code{train.fraction} ratio (the same applies to cross-validation\n",
       "folds). Internally, queries are randomly shuffled before training, to\n",
       "avoid bias.\n",
       "\n",
       "Weights can be used in conjunction with pairwise metrics, however it is\n",
       "assumed that they are constant for instances from the same group.\n",
       "\n",
       "For details and background on the algorithm, see e.g. Burges (2010).\n",
       "\n",
       "\n",
       "\\item[\\code{data}] an optional data frame containing the variables in the model. By default the variables are taken from \\code{environment(formula)}, typically the environment from which \\code{gbm} is called. If \\code{keep.data=TRUE} in the initial call to \\code{gbm} then \\code{gbm} stores a copy with the object. If \\code{keep.data=FALSE} then subsequent calls to \\code{\\LinkA{gbm.more}{gbm.more}} must resupply the same dataset. It becomes the user's responsibility to resupply the same data at this point.\n",
       "\\item[\\code{weights}] an optional vector of weights to be used in the fitting process. Must be positive but do not need to be normalized. If \\code{keep.data=FALSE} in the initial call to \\code{gbm} then it is the user's responsibility to resupply the weights to \\code{\\LinkA{gbm.more}{gbm.more}}.\n",
       "\\item[\\code{var.monotone}] an optional vector, the same length as the number of predictors, indicating which variables have a monotone increasing (+1), decreasing (-1), or arbitrary (0) relationship with the outcome.\n",
       "\\item[\\code{n.trees}] the total number of trees to fit. This is equivalent to the number of iterations and the number of basis functions in the additive expansion.\n",
       "\\item[\\code{cv.folds}] Number of cross-validation folds to perform. If \\code{cv.folds}>1 then \\code{gbm}, in addition to the usual fit, will perform a cross-validation, calculate an estimate of generalization error returned in \\code{cv.error}.\n",
       "\\item[\\code{interaction.depth}] The maximum depth of variable interactions. 1 implies an additive model, 2 implies a model with up to 2-way interactions, etc.\n",
       "\\item[\\code{n.minobsinnode}] minimum number of observations in the trees terminal nodes. Note that this is the actual number of observations not the total weight.\n",
       "\\item[\\code{shrinkage}] a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction.\n",
       "\\item[\\code{bag.fraction}] the fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit. If \\code{bag.fraction}<1 then running the same model twice will result in similar but different fits. \\code{gbm} uses the R random number generator so \\code{set.seed} can ensure that the model can be reconstructed. Preferably, the user can save the returned \\code{\\LinkA{gbm.object}{gbm.object}} using \\code{\\LinkA{save}{save}}.\n",
       "\\item[\\code{train.fraction}] The first \\code{train.fraction * nrows(data)}\n",
       "observations are used to fit the \\code{gbm} and the remainder are used\n",
       "for computing out-of-sample estimates of the loss function.\n",
       "\\item[\\code{nTrain}] An integer representing the number of cases on which to\n",
       "train. This is the preferred way of specification for \\code{gbm.fit};\n",
       "The option \\code{train.fraction} in \\code{gbm.fit} is deprecated and\n",
       "only maintained for backward compatibility. These two parameters are\n",
       "mutually exclusive. If both are unspecified, all data is used for training.\n",
       "\\item[\\code{keep.data}] a logical variable indicating whether to keep the data and an index of the data stored with the object. Keeping the data and index makes subsequent calls to \\code{\\LinkA{gbm.more}{gbm.more}} faster at the cost of storing an extra copy of the dataset.\n",
       "\\item[\\code{object}] a \\code{gbm} object created from an initial call to \\code{\\LinkA{gbm}{gbm}}.\n",
       "\\item[\\code{n.new.trees}] the number of additional trees to add to \\code{object}.\n",
       "\\item[\\code{verbose}] If TRUE, gbm will print out progress and performance indicators. If this option is left unspecified for gbm.more then it uses \\code{verbose} from \\code{object}.\n",
       "\\item[\\code{class.stratify.cv}] whether or not the cross-validation should be stratified by class. Defaults to \\code{TRUE} for \\code{distribution=\"multinomial\"} and is only implementated for \\code{multinomial} and \\code{bernoulli}. The purpose of stratifying the cross-validation is to help avoiding situations in which training sets do not contain all classes.\n",
       "\\item[\\code{x, y}] For \\code{gbm.fit}: \\code{x} is a data frame or data matrix containing the predictor variables and \\code{y} is the vector of outcomes. The number of rows in \\code{x} must be the same as the length of \\code{y}.\n",
       "\\item[\\code{offset}] a vector of values for the offset\n",
       "\\item[\\code{misc}] For \\code{gbm.fit}: \\code{misc} is an R object that is simply passed on to the gbm engine. It can be used for additional data for the specific distribution. Currently it is only used for passing the censoring indicator for the Cox proportional hazards model.\n",
       "\\item[\\code{w}] For \\code{gbm.fit}: \\code{w} is a vector of weights of the same length as the \\code{y}.\n",
       "\\item[\\code{var.names}] For \\code{gbm.fit}: A vector of strings of length equal to the number of columns of \\code{x} containing the names of the predictor variables.\n",
       "\\item[\\code{response.name}] For \\code{gbm.fit}: A character string label for the response variable.\n",
       "\\item[\\code{group}] \\code{group} used when \\code{distribution = 'pairwise'.}\n",
       "\\item[\\code{n.cores}] The number of CPU cores to use. The cross-validation loop\n",
       "will attempt to send different CV folds off to different cores. If\n",
       "\\code{n.cores} is not specified by the user, it is guessed using the\n",
       "\\code{detectCores} function in the \\code{parallel} package. Note that\n",
       "the documentation for \\code{detectCores} makes clear that it is not\n",
       "failsave and could return a spurious number of available cores.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "See the \\Rhref{../doc/gbm.pdf}{gbm vignette} for technical details.\n",
       "\n",
       "This package implements the generalized boosted modeling framework. Boosting is the process of iteratively adding basis functions in a greedy fashion so that each additional basis function further reduces the selected loss function. This implementation closely follows Friedman's Gradient Boosting Machine (Friedman, 2001).\n",
       "\n",
       "In addition to many of the features documented in the Gradient Boosting Machine, \\code{gbm} offers additional features including the out-of-bag estimator for the optimal number of iterations, the ability to store and manipulate the resulting \\code{gbm} object, and a variety of other loss functions that had not previously had associated boosting algorithms, including the Cox partial likelihood for censored data, the poisson likelihood for count outcomes, and a gradient boosting implementation to minimize the AdaBoost exponential loss function.\n",
       "\n",
       "\\code{gbm.fit} provides the link between R and the C++ gbm engine. \\code{gbm} is a front-end to \\code{gbm.fit} that uses the familiar R modeling formulas. However, \\code{\\LinkA{model.frame}{model.frame}} is very slow if there are many predictor variables. For power-users with many variables use \\code{gbm.fit}. For general practice \\code{gbm} is preferable.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       " \\code{gbm}, \\code{gbm.fit}, and \\code{gbm.more} return a \\code{\\LinkA{gbm.object}{gbm.object}}. \n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Greg Ridgeway \\email{gregridgeway@gmail.com}\n",
       "\n",
       "Quantile regression code developed by Brian Kriegler \\email{bk@stat.ucla.edu}\n",
       "\n",
       "t-distribution, and multinomial code developed by Harry Southworth and Daniel Edwards\n",
       "\n",
       "Pairwise code developed by Stefan Schroedl \\email{schroedl@a9.com}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Y. Freund and R.E. Schapire (1997) ``A decision-theoretic generalization of on-line learning and an application to boosting,'' \\emph{Journal of Computer and System Sciences,} 55(1):119-139.\n",
       "\n",
       "G. Ridgeway (1999). ``The state of boosting,'' \\emph{Computing Science and Statistics} 31:172-181.\n",
       "\n",
       "J.H. Friedman, T. Hastie, R. Tibshirani (2000). ``Additive Logistic Regression: a Statistical View of Boosting,'' \\emph{Annals of Statistics} 28(2):337-374.\n",
       "\n",
       "J.H. Friedman (2001). ``Greedy Function Approximation: A Gradient Boosting Machine,'' \\emph{Annals of Statistics} 29(5):1189-1232.\n",
       "\n",
       "J.H. Friedman (2002). ``Stochastic Gradient Boosting,'' \\emph{Computational Statistics and Data Analysis} 38(4):367-378.\n",
       "\n",
       "B. Kriegler (2007). \\Rhref{http://statistics.ucla.edu/theses/uclastat-dissertation-2007:2}{Cost-Sensitive Stochastic Gradient Boosting Within a Quantitative Regression Framework}. PhD dissertation, UCLA Statistics.\n",
       "\n",
       "C. Burges (2010). ``From RankNet to LambdaRank to LambdaMART: An Overview,'' Microsoft Research Technical Report MSR-TR-2010-82.\n",
       "\n",
       "\\Rhref{http://sites.google.com/site/gregridgeway}{Greg Ridgeway's site}.\n",
       "\n",
       "The \\Rhref{http://www-stat.stanford.edu/~jhf/R-MART.html}{MART} website. \n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       " \\code{\\LinkA{gbm.object}{gbm.object}}, \\code{\\LinkA{gbm.perf}{gbm.perf}}, \\code{\\LinkA{plot.gbm}{plot.gbm}},\n",
       "\\code{\\LinkA{predict.gbm}{predict.gbm}}, \\code{\\LinkA{summary.gbm}{summary.gbm}}, \\code{\\LinkA{pretty.gbm.tree}{pretty.gbm.tree}}. \n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       " # A least squares regression example # create some data\n",
       "\n",
       "N <- 1000\n",
       "X1 <- runif(N)\n",
       "X2 <- 2*runif(N)\n",
       "X3 <- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])\n",
       "X4 <- factor(sample(letters[1:6],N,replace=TRUE))\n",
       "X5 <- factor(sample(letters[1:3],N,replace=TRUE))\n",
       "X6 <- 3*runif(N) \n",
       "mu <- c(-1,0,1,2)[as.numeric(X3)]\n",
       "\n",
       "SNR <- 10 # signal-to-noise ratio\n",
       "Y <- X1**1.5 + 2 * (X2**.5) + mu\n",
       "sigma <- sqrt(var(Y)/SNR)\n",
       "Y <- Y + rnorm(N,0,sigma)\n",
       "\n",
       "# introduce some missing values\n",
       "X1[sample(1:N,size=500)] <- NA\n",
       "X4[sample(1:N,size=300)] <- NA\n",
       "\n",
       "data <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
       "\n",
       "# fit initial model\n",
       "gbm1 <-\n",
       "gbm(Y~X1+X2+X3+X4+X5+X6,         # formula\n",
       "    data=data,                   # dataset\n",
       "    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,\n",
       "                                 # +1: monotone increase,\n",
       "                                 #  0: no monotone restrictions\n",
       "    distribution=\"gaussian\",     # see the help for other choices\n",
       "    n.trees=1000,                # number of trees\n",
       "    shrinkage=0.05,              # shrinkage or learning rate,\n",
       "                                 # 0.001 to 0.1 usually work\n",
       "    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.\n",
       "    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best\n",
       "    train.fraction = 0.5,        # fraction of data for training,\n",
       "                                 # first train.fraction*N used for training\n",
       "    n.minobsinnode = 10,         # minimum total weight needed in each node\n",
       "    cv.folds = 3,                # do 3-fold cross-validation\n",
       "    keep.data=TRUE,              # keep a copy of the dataset with the object\n",
       "    verbose=FALSE,               # don't print out progress\n",
       "    n.cores=1)                   # use only a single core (detecting #cores is\n",
       "                                 # error-prone, so avoided here)\n",
       "\n",
       "# check performance using an out-of-bag estimator\n",
       "# OOB underestimates the optimal number of iterations\n",
       "best.iter <- gbm.perf(gbm1,method=\"OOB\")\n",
       "print(best.iter)\n",
       "\n",
       "# check performance using a 50% heldout test set\n",
       "best.iter <- gbm.perf(gbm1,method=\"test\")\n",
       "print(best.iter)\n",
       "\n",
       "# check performance using 5-fold cross-validation\n",
       "best.iter <- gbm.perf(gbm1,method=\"cv\")\n",
       "print(best.iter)\n",
       "\n",
       "# plot the performance # plot variable influence\n",
       "summary(gbm1,n.trees=1)         # based on the first tree\n",
       "summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees\n",
       "\n",
       "# compactly print the first and last trees for curiosity\n",
       "print(pretty.gbm.tree(gbm1,1))\n",
       "print(pretty.gbm.tree(gbm1,gbm1$n.trees))\n",
       "\n",
       "# make some new data\n",
       "N <- 1000\n",
       "X1 <- runif(N)\n",
       "X2 <- 2*runif(N)\n",
       "X3 <- ordered(sample(letters[1:4],N,replace=TRUE))\n",
       "X4 <- factor(sample(letters[1:6],N,replace=TRUE))\n",
       "X5 <- factor(sample(letters[1:3],N,replace=TRUE))\n",
       "X6 <- 3*runif(N) \n",
       "mu <- c(-1,0,1,2)[as.numeric(X3)]\n",
       "\n",
       "Y <- X1**1.5 + 2 * (X2**.5) + mu + rnorm(N,0,sigma)\n",
       "\n",
       "data2 <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
       "\n",
       "# predict on the new data using \"best\" number of trees\n",
       "# f.predict generally will be on the canonical scale (logit,log,etc.)\n",
       "f.predict <- predict(gbm1,data2,best.iter)\n",
       "\n",
       "# least squares error\n",
       "print(sum((data2$Y-f.predict)^2))\n",
       "\n",
       "# create marginal plots\n",
       "# plot variable X1,X2,X3 after \"best\" iterations\n",
       "par(mfrow=c(1,3))\n",
       "plot(gbm1,1,best.iter)\n",
       "plot(gbm1,2,best.iter)\n",
       "plot(gbm1,3,best.iter)\n",
       "par(mfrow=c(1,1))\n",
       "# contour plot of variables 1 and 2 after \"best\" iterations\n",
       "plot(gbm1,1:2,best.iter)\n",
       "# lattice plot of variables 2 and 3\n",
       "plot(gbm1,2:3,best.iter)\n",
       "# lattice plot of variables 3 and 4\n",
       "plot(gbm1,3:4,best.iter)\n",
       "\n",
       "# 3-way plots\n",
       "plot(gbm1,c(1,2,6),best.iter,cont=20)\n",
       "plot(gbm1,1:3,best.iter)\n",
       "plot(gbm1,2:4,best.iter)\n",
       "plot(gbm1,3:5,best.iter)\n",
       "\n",
       "# do another 100 iterations\n",
       "gbm2 <- gbm.more(gbm1,100,\n",
       "                 verbose=FALSE) # stop printing detailed progress\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "gbm                    package:gbm                     R Documentation\n",
       "\n",
       "_\bG_\be_\bn_\be_\br_\ba_\bl_\bi_\bz_\be_\bd _\bB_\bo_\bo_\bs_\bt_\be_\bd _\bR_\be_\bg_\br_\be_\bs_\bs_\bi_\bo_\bn _\bM_\bo_\bd_\be_\bl_\bi_\bn_\bg\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Fits generalized boosted regression models.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     gbm(formula = formula(data),\n",
       "         distribution = \"bernoulli\",\n",
       "         data = list(),\n",
       "         weights,\n",
       "         var.monotone = NULL,\n",
       "         n.trees = 100,\n",
       "         interaction.depth = 1,\n",
       "         n.minobsinnode = 10,\n",
       "         shrinkage = 0.001,\n",
       "         bag.fraction = 0.5,\n",
       "         train.fraction = 1.0,\n",
       "         cv.folds=0,\n",
       "         keep.data = TRUE,\n",
       "         verbose = \"CV\",\n",
       "         class.stratify.cv=NULL,\n",
       "         n.cores = NULL)\n",
       "     \n",
       "     gbm.fit(x, y,\n",
       "             offset = NULL,\n",
       "             misc = NULL,\n",
       "             distribution = \"bernoulli\",\n",
       "             w = NULL,\n",
       "             var.monotone = NULL,\n",
       "             n.trees = 100,\n",
       "             interaction.depth = 1,\n",
       "             n.minobsinnode = 10,\n",
       "             shrinkage = 0.001,\n",
       "             bag.fraction = 0.5,\n",
       "             nTrain = NULL,\n",
       "             train.fraction = NULL,\n",
       "             keep.data = TRUE,\n",
       "             verbose = TRUE,\n",
       "             var.names = NULL,\n",
       "             response.name = \"y\",\n",
       "             group = NULL)\n",
       "     \n",
       "     gbm.more(object,\n",
       "              n.new.trees = 100,\n",
       "              data = NULL,\n",
       "              weights = NULL,\n",
       "              offset = NULL,\n",
       "              verbose = NULL)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       " formula: a symbolic description of the model to be fit. The formula\n",
       "          may include an offset term (e.g. y~offset(n)+x). If\n",
       "          'keep.data=FALSE' in the initial call to 'gbm' then it is the\n",
       "          user's responsibility to resupply the offset to 'gbm.more'.\n",
       "\n",
       "distribution: either a character string specifying the name of the\n",
       "          distribution to use or a list with a component 'name'\n",
       "          specifying the distribution and any additional parameters\n",
       "          needed. If not specified, 'gbm' will try to guess: if the\n",
       "          response has only 2 unique values, bernoulli is assumed;\n",
       "          otherwise, if the response is a factor, multinomial is\n",
       "          assumed; otherwise, if the response has class \"Surv\", coxph\n",
       "          is assumed; otherwise, gaussian is assumed.\n",
       "\n",
       "          Currently available options are \"gaussian\" (squared error),\n",
       "          \"laplace\" (absolute loss), \"tdist\" (t-distribution loss),\n",
       "          \"bernoulli\" (logistic regression for 0-1 outcomes),\n",
       "          \"huberized\" (huberized hinge loss for 0-1 outcomes),\n",
       "          \"multinomial\" (classification when there are more than 2\n",
       "          classes), \"adaboost\" (the AdaBoost exponential loss for 0-1\n",
       "          outcomes), \"poisson\" (count outcomes), \"coxph\" (right\n",
       "          censored observations), \"quantile\", or \"pairwise\" (ranking\n",
       "          measure using the LambdaMart algorithm).\n",
       "\n",
       "          If quantile regression is specified, 'distribution' must be a\n",
       "          list of the form 'list(name=\"quantile\",alpha=0.25)' where\n",
       "          'alpha' is the quantile to estimate. The current version's\n",
       "          quantile regression method does not handle non-constant\n",
       "          weights and will stop.\n",
       "\n",
       "          If \"tdist\" is specified, the default degrees of freedom is 4\n",
       "          and this can be controlled by specifying\n",
       "          'distribution=list(name=\"tdist\", df=DF)' where 'DF' is your\n",
       "          chosen degrees of freedom.\n",
       "\n",
       "          If \"pairwise\" regression is specified, 'distribution' must be\n",
       "          a list of the form\n",
       "          'list(name=\"pairwise\",group=...,metric=...,max.rank=...)'\n",
       "          ('metric' and 'max.rank' are optional, see below). 'group' is\n",
       "          a character vector with the column names of 'data' that\n",
       "          jointly indicate the group an instance belongs to (typically\n",
       "          a query in Information Retrieval applications). For training,\n",
       "          only pairs of instances from the same group and with\n",
       "          different target labels can be considered. 'metric' is the IR\n",
       "          measure to use, one of\n",
       "\n",
       "          'conc': Fraction of concordant pairs; for binary labels, this\n",
       "              is equivalent to the Area under the ROC Curve\n",
       "\n",
       "          'mrr': Mean reciprocal rank of the highest-ranked positive\n",
       "              instance\n",
       "\n",
       "          'map': Mean average precision, a generalization of 'mrr' to\n",
       "              multiple positive instances\n",
       "\n",
       "          'ndcg:' Normalized discounted cumulative gain. The score is\n",
       "              the weighted sum (DCG) of the user-supplied target\n",
       "              values, weighted by log(rank+1), and normalized to the\n",
       "              maximum achievable value. This is the default if the user\n",
       "              did not specify a metric.\n",
       "\n",
       "          'ndcg' and 'conc' allow arbitrary target values, while binary\n",
       "          targets {0,1} are expected for 'map' and 'mrr'. For 'ndcg'\n",
       "          and 'mrr', a cut-off can be chosen using a positive integer\n",
       "          parameter 'max.rank'. If left unspecified, all ranks are\n",
       "          taken into account.\n",
       "\n",
       "          Note that splitting of instances into training and validation\n",
       "          sets follows group boundaries and therefore only approximates\n",
       "          the specified 'train.fraction' ratio (the same applies to\n",
       "          cross-validation folds). Internally, queries are randomly\n",
       "          shuffled before training, to avoid bias.\n",
       "\n",
       "          Weights can be used in conjunction with pairwise metrics,\n",
       "          however it is assumed that they are constant for instances\n",
       "          from the same group.\n",
       "\n",
       "          For details and background on the algorithm, see e.g. Burges\n",
       "          (2010).\n",
       "\n",
       "    data: an optional data frame containing the variables in the model.\n",
       "          By default the variables are taken from\n",
       "          'environment(formula)', typically the environment from which\n",
       "          'gbm' is called. If 'keep.data=TRUE' in the initial call to\n",
       "          'gbm' then 'gbm' stores a copy with the object. If\n",
       "          'keep.data=FALSE' then subsequent calls to 'gbm.more' must\n",
       "          resupply the same dataset. It becomes the user's\n",
       "          responsibility to resupply the same data at this point.\n",
       "\n",
       " weights: an optional vector of weights to be used in the fitting\n",
       "          process. Must be positive but do not need to be normalized.\n",
       "          If 'keep.data=FALSE' in the initial call to 'gbm' then it is\n",
       "          the user's responsibility to resupply the weights to\n",
       "          'gbm.more'.\n",
       "\n",
       "var.monotone: an optional vector, the same length as the number of\n",
       "          predictors, indicating which variables have a monotone\n",
       "          increasing (+1), decreasing (-1), or arbitrary (0)\n",
       "          relationship with the outcome.\n",
       "\n",
       " n.trees: the total number of trees to fit. This is equivalent to the\n",
       "          number of iterations and the number of basis functions in the\n",
       "          additive expansion.\n",
       "\n",
       "cv.folds: Number of cross-validation folds to perform. If 'cv.folds'>1\n",
       "          then 'gbm', in addition to the usual fit, will perform a\n",
       "          cross-validation, calculate an estimate of generalization\n",
       "          error returned in 'cv.error'.\n",
       "\n",
       "interaction.depth: The maximum depth of variable interactions. 1\n",
       "          implies an additive model, 2 implies a model with up to 2-way\n",
       "          interactions, etc.\n",
       "\n",
       "n.minobsinnode: minimum number of observations in the trees terminal\n",
       "          nodes. Note that this is the actual number of observations\n",
       "          not the total weight.\n",
       "\n",
       "shrinkage: a shrinkage parameter applied to each tree in the expansion.\n",
       "          Also known as the learning rate or step-size reduction.\n",
       "\n",
       "bag.fraction: the fraction of the training set observations randomly\n",
       "          selected to propose the next tree in the expansion. This\n",
       "          introduces randomnesses into the model fit. If\n",
       "          'bag.fraction'<1 then running the same model twice will\n",
       "          result in similar but different fits. 'gbm' uses the R random\n",
       "          number generator so 'set.seed' can ensure that the model can\n",
       "          be reconstructed. Preferably, the user can save the returned\n",
       "          'gbm.object' using 'save'.\n",
       "\n",
       "train.fraction: The first 'train.fraction * nrows(data)' observations\n",
       "          are used to fit the 'gbm' and the remainder are used for\n",
       "          computing out-of-sample estimates of the loss function.\n",
       "\n",
       "  nTrain: An integer representing the number of cases on which to\n",
       "          train. This is the preferred way of specification for\n",
       "          'gbm.fit'; The option 'train.fraction' in 'gbm.fit' is\n",
       "          deprecated and only maintained for backward compatibility.\n",
       "          These two parameters are mutually exclusive. If both are\n",
       "          unspecified, all data is used for training.\n",
       "\n",
       "keep.data: a logical variable indicating whether to keep the data and\n",
       "          an index of the data stored with the object. Keeping the data\n",
       "          and index makes subsequent calls to 'gbm.more' faster at the\n",
       "          cost of storing an extra copy of the dataset.\n",
       "\n",
       "  object: a 'gbm' object created from an initial call to 'gbm'.\n",
       "\n",
       "n.new.trees: the number of additional trees to add to 'object'.\n",
       "\n",
       " verbose: If TRUE, gbm will print out progress and performance\n",
       "          indicators. If this option is left unspecified for gbm.more\n",
       "          then it uses 'verbose' from 'object'.\n",
       "\n",
       "class.stratify.cv: whether or not the cross-validation should be\n",
       "          stratified by class. Defaults to 'TRUE' for\n",
       "          'distribution=\"multinomial\"' and is only implementated for\n",
       "          'multinomial' and 'bernoulli'. The purpose of stratifying the\n",
       "          cross-validation is to help avoiding situations in which\n",
       "          training sets do not contain all classes.\n",
       "\n",
       "    x, y: For 'gbm.fit': 'x' is a data frame or data matrix containing\n",
       "          the predictor variables and 'y' is the vector of outcomes.\n",
       "          The number of rows in 'x' must be the same as the length of\n",
       "          'y'.\n",
       "\n",
       "  offset: a vector of values for the offset\n",
       "\n",
       "    misc: For 'gbm.fit': 'misc' is an R object that is simply passed on\n",
       "          to the gbm engine. It can be used for additional data for the\n",
       "          specific distribution. Currently it is only used for passing\n",
       "          the censoring indicator for the Cox proportional hazards\n",
       "          model.\n",
       "\n",
       "       w: For 'gbm.fit': 'w' is a vector of weights of the same length\n",
       "          as the 'y'.\n",
       "\n",
       "var.names: For 'gbm.fit': A vector of strings of length equal to the\n",
       "          number of columns of 'x' containing the names of the\n",
       "          predictor variables.\n",
       "\n",
       "response.name: For 'gbm.fit': A character string label for the response\n",
       "          variable.\n",
       "\n",
       "   group: 'group' used when 'distribution = 'pairwise'.'\n",
       "\n",
       " n.cores: The number of CPU cores to use. The cross-validation loop\n",
       "          will attempt to send different CV folds off to different\n",
       "          cores. If 'n.cores' is not specified by the user, it is\n",
       "          guessed using the 'detectCores' function in the 'parallel'\n",
       "          package. Note that the documentation for 'detectCores' makes\n",
       "          clear that it is not failsave and could return a spurious\n",
       "          number of available cores.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     See the gbm vignette for technical details.\n",
       "\n",
       "     This package implements the generalized boosted modeling\n",
       "     framework. Boosting is the process of iteratively adding basis\n",
       "     functions in a greedy fashion so that each additional basis\n",
       "     function further reduces the selected loss function. This\n",
       "     implementation closely follows Friedman's Gradient Boosting\n",
       "     Machine (Friedman, 2001).\n",
       "\n",
       "     In addition to many of the features documented in the Gradient\n",
       "     Boosting Machine, 'gbm' offers additional features including the\n",
       "     out-of-bag estimator for the optimal number of iterations, the\n",
       "     ability to store and manipulate the resulting 'gbm' object, and a\n",
       "     variety of other loss functions that had not previously had\n",
       "     associated boosting algorithms, including the Cox partial\n",
       "     likelihood for censored data, the poisson likelihood for count\n",
       "     outcomes, and a gradient boosting implementation to minimize the\n",
       "     AdaBoost exponential loss function.\n",
       "\n",
       "     'gbm.fit' provides the link between R and the C++ gbm engine.\n",
       "     'gbm' is a front-end to 'gbm.fit' that uses the familiar R\n",
       "     modeling formulas. However, 'model.frame' is very slow if there\n",
       "     are many predictor variables. For power-users with many variables\n",
       "     use 'gbm.fit'. For general practice 'gbm' is preferable.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     'gbm', 'gbm.fit', and 'gbm.more' return a 'gbm.object'.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Greg Ridgeway <email: gregridgeway@gmail.com>\n",
       "\n",
       "     Quantile regression code developed by Brian Kriegler <email:\n",
       "     bk@stat.ucla.edu>\n",
       "\n",
       "     t-distribution, and multinomial code developed by Harry Southworth\n",
       "     and Daniel Edwards\n",
       "\n",
       "     Pairwise code developed by Stefan Schroedl <email:\n",
       "     schroedl@a9.com>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Y. Freund and R.E. Schapire (1997) \"A decision-theoretic\n",
       "     generalization of on-line learning and an application to\n",
       "     boosting,\" _Journal of Computer and System Sciences,_\n",
       "     55(1):119-139.\n",
       "\n",
       "     G. Ridgeway (1999). \"The state of boosting,\" _Computing Science\n",
       "     and Statistics_ 31:172-181.\n",
       "\n",
       "     J.H. Friedman, T. Hastie, R. Tibshirani (2000). \"Additive Logistic\n",
       "     Regression: a Statistical View of Boosting,\" _Annals of\n",
       "     Statistics_ 28(2):337-374.\n",
       "\n",
       "     J.H. Friedman (2001). \"Greedy Function Approximation: A Gradient\n",
       "     Boosting Machine,\" _Annals of Statistics_ 29(5):1189-1232.\n",
       "\n",
       "     J.H. Friedman (2002). \"Stochastic Gradient Boosting,\"\n",
       "     _Computational Statistics and Data Analysis_ 38(4):367-378.\n",
       "\n",
       "     B. Kriegler (2007). Cost-Sensitive Stochastic Gradient Boosting\n",
       "     Within a Quantitative Regression Framework. PhD dissertation, UCLA\n",
       "     Statistics.\n",
       "\n",
       "     C. Burges (2010). \"From RankNet to LambdaRank to LambdaMART: An\n",
       "     Overview,\" Microsoft Research Technical Report MSR-TR-2010-82.\n",
       "\n",
       "     Greg Ridgeway's site.\n",
       "\n",
       "     The MART website.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     'gbm.object', 'gbm.perf', 'plot.gbm', 'predict.gbm',\n",
       "     'summary.gbm', 'pretty.gbm.tree'.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "      # A least squares regression example # create some data\n",
       "     \n",
       "     N <- 1000\n",
       "     X1 <- runif(N)\n",
       "     X2 <- 2*runif(N)\n",
       "     X3 <- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])\n",
       "     X4 <- factor(sample(letters[1:6],N,replace=TRUE))\n",
       "     X5 <- factor(sample(letters[1:3],N,replace=TRUE))\n",
       "     X6 <- 3*runif(N) \n",
       "     mu <- c(-1,0,1,2)[as.numeric(X3)]\n",
       "     \n",
       "     SNR <- 10 # signal-to-noise ratio\n",
       "     Y <- X1**1.5 + 2 * (X2**.5) + mu\n",
       "     sigma <- sqrt(var(Y)/SNR)\n",
       "     Y <- Y + rnorm(N,0,sigma)\n",
       "     \n",
       "     # introduce some missing values\n",
       "     X1[sample(1:N,size=500)] <- NA\n",
       "     X4[sample(1:N,size=300)] <- NA\n",
       "     \n",
       "     data <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
       "     \n",
       "     # fit initial model\n",
       "     gbm1 <-\n",
       "     gbm(Y~X1+X2+X3+X4+X5+X6,         # formula\n",
       "         data=data,                   # dataset\n",
       "         var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,\n",
       "                                      # +1: monotone increase,\n",
       "                                      #  0: no monotone restrictions\n",
       "         distribution=\"gaussian\",     # see the help for other choices\n",
       "         n.trees=1000,                # number of trees\n",
       "         shrinkage=0.05,              # shrinkage or learning rate,\n",
       "                                      # 0.001 to 0.1 usually work\n",
       "         interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.\n",
       "         bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best\n",
       "         train.fraction = 0.5,        # fraction of data for training,\n",
       "                                      # first train.fraction*N used for training\n",
       "         n.minobsinnode = 10,         # minimum total weight needed in each node\n",
       "         cv.folds = 3,                # do 3-fold cross-validation\n",
       "         keep.data=TRUE,              # keep a copy of the dataset with the object\n",
       "         verbose=FALSE,               # don't print out progress\n",
       "         n.cores=1)                   # use only a single core (detecting #cores is\n",
       "                                      # error-prone, so avoided here)\n",
       "     \n",
       "     # check performance using an out-of-bag estimator\n",
       "     # OOB underestimates the optimal number of iterations\n",
       "     best.iter <- gbm.perf(gbm1,method=\"OOB\")\n",
       "     print(best.iter)\n",
       "     \n",
       "     # check performance using a 50% heldout test set\n",
       "     best.iter <- gbm.perf(gbm1,method=\"test\")\n",
       "     print(best.iter)\n",
       "     \n",
       "     # check performance using 5-fold cross-validation\n",
       "     best.iter <- gbm.perf(gbm1,method=\"cv\")\n",
       "     print(best.iter)\n",
       "     \n",
       "     # plot the performance # plot variable influence\n",
       "     summary(gbm1,n.trees=1)         # based on the first tree\n",
       "     summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees\n",
       "     \n",
       "     # compactly print the first and last trees for curiosity\n",
       "     print(pretty.gbm.tree(gbm1,1))\n",
       "     print(pretty.gbm.tree(gbm1,gbm1$n.trees))\n",
       "     \n",
       "     # make some new data\n",
       "     N <- 1000\n",
       "     X1 <- runif(N)\n",
       "     X2 <- 2*runif(N)\n",
       "     X3 <- ordered(sample(letters[1:4],N,replace=TRUE))\n",
       "     X4 <- factor(sample(letters[1:6],N,replace=TRUE))\n",
       "     X5 <- factor(sample(letters[1:3],N,replace=TRUE))\n",
       "     X6 <- 3*runif(N) \n",
       "     mu <- c(-1,0,1,2)[as.numeric(X3)]\n",
       "     \n",
       "     Y <- X1**1.5 + 2 * (X2**.5) + mu + rnorm(N,0,sigma)\n",
       "     \n",
       "     data2 <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)\n",
       "     \n",
       "     # predict on the new data using \"best\" number of trees\n",
       "     # f.predict generally will be on the canonical scale (logit,log,etc.)\n",
       "     f.predict <- predict(gbm1,data2,best.iter)\n",
       "     \n",
       "     # least squares error\n",
       "     print(sum((data2$Y-f.predict)^2))\n",
       "     \n",
       "     # create marginal plots\n",
       "     # plot variable X1,X2,X3 after \"best\" iterations\n",
       "     par(mfrow=c(1,3))\n",
       "     plot(gbm1,1,best.iter)\n",
       "     plot(gbm1,2,best.iter)\n",
       "     plot(gbm1,3,best.iter)\n",
       "     par(mfrow=c(1,1))\n",
       "     # contour plot of variables 1 and 2 after \"best\" iterations\n",
       "     plot(gbm1,1:2,best.iter)\n",
       "     # lattice plot of variables 2 and 3\n",
       "     plot(gbm1,2:3,best.iter)\n",
       "     # lattice plot of variables 3 and 4\n",
       "     plot(gbm1,3:4,best.iter)\n",
       "     \n",
       "     # 3-way plots\n",
       "     plot(gbm1,c(1,2,6),best.iter,cont=20)\n",
       "     plot(gbm1,1:3,best.iter)\n",
       "     plot(gbm1,2:4,best.iter)\n",
       "     plot(gbm1,3:5,best.iter)\n",
       "     \n",
       "     # do another 100 iterations\n",
       "     gbm2 <- gbm.more(gbm1,100,\n",
       "                      verbose=FALSE) # stop printing detailed progress\n",
       "     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
